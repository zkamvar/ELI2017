\RequirePackage{nag}
\documentclass[12pt,letterpaper]{article}
\usepackage{lipsum}
\input{config.tex}

\title{\ruleline{Facilities \& Other Resources}}
\lhead{Zhian N. Kamvar, Ph. D.}
\rhead{Facilities \& Other Resources}

\begin{document}
\maketitle

\section{Laboratory}

The primary mentor's laboratory (approximately 756 square feet)---located in Plant Sciences Hall on the University of Nebraska-Lincoln (UNL)'s east campus---is fully equipped for microbiology and molecular biology studies. It has two large sinks, an attached cold room, and is adjacent to the autoclave room. It has access points for computer networking, and has gas and air lines and centrally distilled water.

\section{Office}

The PD's office (approximately 70 square feet of recently refurbished space) is also located in Plant Sciences Hall, where the majority of the faculty members of the Department of Plant Pathology are housed. The office is equipped with a fully networked Macintosh laptop computer that is connected to external monitors and color printer. The members of the Primary Mentor's laboratory are provided with their own office space and computers in close proximity to the both the laboratory and the PD's office.

\section{Additional Campus Resources Relevant to the Proposed Work}

\subsection{UNL Core Facilities}

Computing resources at UNL are provided by the following core facilities:

\begin{itemize}
	\item The Holland Computing Center (HCC) is a high-performance computing
	cluster at UNL, collectively containing 20,000 cores, 3 Petabytes work
	storage, and 100+ Gbps transfer speed. HCC has two machine room locations
	directly interconnected by a pair of 10 Gbps fiber optic links (20 Gbps
	total). An 1,800-square-foot machine room at the Peter Kiewit Institute
	(PKI) in Omaha provides up to 500 kVA in UPS/genset protected power, and 160
	ton cooling. A 2,200-square-foot machine room in Schorr Center at UNL
	provides up to 100 ton cooling with up to 400 kVA of power.
	\begin{itemize}
		\item Servers at PKI connect to campus and Internet2/ESnet at 10 Gbps, with the largest HCC clusters: Tusker and Crane. Tusker has 6,784 cores interconnected with Mellanox QDR Infiniband and 523TB Lustre storage. Each node is an R815 server with 256+ GB RAM and 4 Opteron 6272 (2.1 GHz) processors. Crane has an HPL benchmark of 121.8 TeraFLOPS, with Intel Xeon chips (8-core, 2.6 GHz), and 4 GB RAM available on each core. The cluster shares 1.5 Petabytes of lustre storage.
		\item The Schorr machine room at UNL connects to campus and Internet2/ESnet at 100 Gbps. There are two clusters at UNL: Sandhills and Red. Sandhills is a Linux cluster dedicated to general campus usage with 5,472 compute cores interconnected by low-latency Infiniband networking. There is 175 TB of Lustre storage, 50 TB of NFS storage, and 3 TB of local scratch per node. Red contains 5,696 cores interconnected by 1 and 10 Gb Ethernet and serves over 4.4 PB of storage using Hadoop Distributed File System.
		\item Attic and Silo (backup mirror to Attic) form a near line archive with 1 PB of usable storage. Attic is located at PKI and Silo acts as an online backup in Lincoln. Both Attic and Silo are connected with 10Gbps network connections.
	\end{itemize}
\end{itemize}

\end{document}
